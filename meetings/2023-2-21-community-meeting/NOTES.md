**Notes** ([Recording](https://youtu.be/6x5NSDg3VEI) | [Slides](https://docs.google.com/presentation/d/1U6ZrfeOMVivSz-wzbfQciAVGTgxULKexoA7UWRbsLDA/))
- Eric Kunze (ARM): 
  - **Introduction:** This is meant to be an introduction to the TOSA RFC for feedback from the community. TOSA provides whole tensor operations commonly employed by Deep Neural Networks. Implementable across multiple hardware accelerators. Check out the RFC to provide feedback.
  - **Background:** An overview of Quantization. A topic for discussion is fully quantized networks. The goal being to deploy a program that is as low power as possible. In these networks all operations are integer. Reproducibility across multiple implementations is not possible with floating point.
  - **TOSA Precision Scaling:** see link the presentation for more details
  - **Explicit Example of Quantized Addition:** A lot of times what happens in the framework is hidden. You don’t know exactly how the add operation is taking place. We break this addition out. TOSA uses RESCALE to make the operation explicit.
  - **Proposal:** See link in the presentation. They are suggesting that we align the StableHLO quantized operations to be implementable with TOSA operations. Value to the community - run StableHLO models on TOSA compliant hardware with accurate behavior. Primarily means adding a RESCALE operation that is effectively very similar to the TOSA operator. Go through the same process to remove the floating point from the network. Add a pass that moves the quantization parameters (scale/zp) to the integer based rescale operation. Current StableHLO spec, isn’t explicit when you have quantized tensors. It would be good to have that written down. In the RFC there are some open issues such as (1) 8-bit floating point (2) FP8 support (3) Dynamic quantization scaling - better alignment needed (4) Floating point compound operator scaling - TOSA describes a method for calculating allowable error bound for compound floating-point operations such as convolutions.
- AG Ramesh (Intel): Does TOSA support per channel quantization also?
  - Eric Kunze: Yes, we do support per channel quantization. You can provide values per channel. 
- Eugene Burmako (Google): (1) Question on current state of quantization in StableHLO (2) Next steps on the RFC. First of all StableHLO spec, it doesn’t talk about quantization right now, but it does support quant uniform types. Inherited this from MHLO. This RFC is very timely - recently we decided to dedicate some time to addressing this this quarter. Speaking of next steps, we haven’t yet bootstrapped the OpenXLA Governance - encourage everyone to comment on the GitHub PR. Depending on the ongoing discussion, we may want to have a follow-up community meeting just on quantization. 
  - Can you talk about your experience with numerical accuracy? Have you observed some differences between the original program and the result and if yes, how big those differences were?
  - Eric Kunze: Yes we’ve gone through. As we add TF legalization to TOSA, we have observed inaccuracies. What we’ve done in those cases, generally, it hasn’t been with the legalizations, and just been a difference in what those legalizations are. Often it’s just a bug in the legalization. If there’s something that we think would be better defined by changing the behavior. Not necessarily based on TOSA. Generally, we have been able to match every TFLite operator behavior. 
- Stella Laurenzo (Google): Good distinction in the quantization discussions. Typically, it’s about how to encode metadata whereas this is defining the end point of a lowering for eliminating such metadata and describes how to run pure integer arithmetic. One is how you represent, floating point metadata. Then there’s the process for how you lower that to a real pure integer opset, which is what in the TFLite TOSA case that Arm has done. Then there is what are the ops that StableHLO needs. Ideally we solve for all three of those. It’s about the ground truth that we really need to solve for here.
- Vinod Grover (NVIDIA): Does the proposal also require xla to be able understand toas.arith dialect?
  - Eric Kunze: No it wouldn’t be required as part of this proposal. It wouldn’t have to understand that. You would either have a legalization from StableHLO to TOSA. In general, I don’t think that’s true. 
  - Vinod Grover: Intent is to do 1:1 translation between the two. 
  - Eric Kunze: For these quantized networks, yes.At the lowest level where you’re heading towards hardware.
- Eugene Burmako Encourage folks to comment on the RFC in GitHub.
- Aart Bik (Google): Discussing new RFC on Sparsity. Want to discuss the high point on the RFC. Discuss on the pain points.
  - We’re seeing more and more that Sparse tensor are becoming wide spread.
  - A good time to start formalizing and making this a bit more precise. A lot of people have different ideas on how to treat sparsity.
  - If we as a community can agree on the spec of sparsity in the StableHLO IR - there is a good opportunity here.Please check out the link on GitHub.
  - Rather than going into on how defining Sparsity, we just propose adding sparsity to the StableHLO specification.
  - **Philosophy:** Sparsity should not be something that we deal with explicitly, but just be a type of the tensors. Tell the compiler that it is sparse and let the compiler handle the sparsity. Sparse compilation is not part of this RFC and more focused on StableHLO.
  - **RFC:** Focuses on two concepts: (1) Sparse tensor types (in addition to regular dense tensor types) (2) StableHLO operations that support sparse tensor types.
  - **Sparse Tensor Types:** We can add any annotation to a tensor type to make it sparse. Formulation is very powerful in defining a wide range of sparse tensor schemes: (1) block sparsity (2) diagonal storage. With this formalism, we will have enough flexibility that we can represent an enormous number of sparse tensor schemes. Wren Romano is working with getting a syntax for this. We think it is time to come up with an even more elegant syntax. There are more examples in the RFC.
  - **Operations:** Idea is that all the operations in the StableHLO specification can be made sparse by defining one of two operands as sparse. So let’s take an example: suppose you add two tensors, suppose you wanted to incorporate sparsity in the operation e.g. StableHLO add for the dense case, the result may sparse, other operands may be sparse. That can get out of hand quickly. With this proposal, using the same operation, but changing the types you really avoid that explosion of opportunities. 
  - **Considerations for Operations Support:** Do we want all operations to accept sparse tensor types or do we only want a restricted set of operations. We’re leaning towards the closed route. We start with a subset operations so we can precisely define sparsity in the spec. We’re open for debate on the open world route. Previous experience with MHLO shows that we don’t want to go overboard in letting every operand, because it’s not always sure what you won’t to do there. This is part of debate in RFC. Second part is: do we want a type inference system - do we want to compile to assume the type. We’re leaning towards having a explicit builders that define sparsity on the input and output types. Leaning towards closed world model and explicit builders instead of inference of sparsity.
- Vinod Grover: so in this RFC any operator defined for dense tensor is defined for a sparse annotated tensor type as well?
  - Aart: Yes so the open rule would allow that. We think that is very ambitious and we are proposing a closed world one: a subset of operations where sparsity makes sense. We are leaning towards this closed world mode. For example, reshaping operations doesn’t make sense with sparsity.
  - Vinod: It sounds like the rationale for this needs to be stated. One rationale is it makes StableHLO simpler, another is it makes the user programs more natural. If it’s both than that’s the best way to do it. I can see advantages of open world. Just annotate operations. 
  - Aart: The open world would correspond to any numpy program. That makes it simpler on the user side. Closed world corresponds to JAX approach. We fill that one is better because we can guarantee to map operations to efficient computations. There’s trade-offs.
  - Vinod: In the input language JAX or PyTorch, we have one way to do it and StableHLO we have closed world. Then StableHLO errors need to be communicated to the source language and user may not understand it. 
  - Aart: I imagine there is a certain protocol on defining builders from a language to StableHLO. If you don’t have builders, you can’t even write the front-end. There’s no builders of sparsity. 
  - Thea Lamkin(Google): Please check out the RFC and dig-in for further commentary
